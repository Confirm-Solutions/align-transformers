{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial experiments with learning pytorch\n",
    "Starting with Atticus' hierarchical equality code!\n",
    "Stolen from https://github.com/atticusg/InterchangeInterventions/blob/main/iit_equality.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import utilities\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "from LIM_deep_neural_classifier import LIMDeepNeuralClassifier\n",
    "import dataset_equality\n",
    "from trainer import LIMTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities.fix_random_seeds()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intervention graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_A(ex, intervention):\n",
    "    graph = {}\n",
    "    for i, obj in enumerate(ex):\n",
    "        graph[\"input\" + str(i+1)] = obj\n",
    "    if \"V1\" in intervention:\n",
    "        graph[\"V1\"] = intervention[\"V1\"]\n",
    "    else:\n",
    "        graph[\"V1\"] = graph[\"input1\"] == graph[\"input2\"]\n",
    "    if \"V2\" in intervention:\n",
    "        graph[\"V2\"] = intervention[\"V2\"]\n",
    "    else:\n",
    "        graph[\"V2\"] = graph[\"input3\"] == graph[\"input4\"]\n",
    "    graph[\"output\"] = graph[\"V1\"] == graph[\"V2\"]\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input1': 'square',\n",
       " 'input2': 'pentagon',\n",
       " 'input3': 'triangle',\n",
       " 'input4': 'triangle',\n",
       " 'V1': True,\n",
       " 'V2': True,\n",
       " 'output': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_A(\n",
    "    (\"pentagon\", \"pentagon\", \"triangle\", \"square\"), \n",
    "    intervention={})\n",
    "\n",
    "compute_A(\n",
    "    (\"square\", \"pentagon\", \"triangle\", \"triangle\"), \n",
    "    intervention={\"V1\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_interchange_A(base, source, variable):\n",
    "    # Run the algorithm on `source`:\n",
    "    src_output = compute_A(source, intervention={})\n",
    "    # Get the source value for `variable`:\n",
    "    val = src_output[variable]\n",
    "    # Process `base` with the intervention setting `variable`\n",
    "    # to the value it had in `source`:        \n",
    "    return compute_A(base, intervention={variable: val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input1': 'pentagon',\n",
       " 'input2': 'pentagon',\n",
       " 'input3': 'triangle',\n",
       " 'input4': 'square',\n",
       " 'V1': False,\n",
       " 'V2': False,\n",
       " 'output': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_interchange_A(\n",
    "    base=(\"pentagon\", \"pentagon\", \"triangle\", \"square\"),    # base: T F ==> F\n",
    "    source=(\"square\", \"pentagon\", \"triangle\", \"triangle\"),  # source: F T ==> F\n",
    "    variable=\"V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 4\n",
    "\n",
    "n_examples = 100000\n",
    "\n",
    "X_train, X_test, y_train, y_test, test_dataset = dataset_equality.get_equality_dataset(\n",
    "    embedding_dim, n_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.2756, -0.4622,  0.3554,  0.0986,  0.2756, -0.4622,  0.3554,  0.0986,\n",
       "          0.3294,  0.4381, -0.2060, -0.2393,  0.4394,  0.2022, -0.3943, -0.1066],\n",
       "        dtype=torch.float64),\n",
       " 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left = torch.equal(\n",
    "    X_train[0][: embedding_dim],\n",
    "    X_train[0][embedding_dim: embedding_dim*2])\n",
    "\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right = torch.equal(\n",
    "    X_train[0][embedding_dim*2: embedding_dim*3],\n",
    "    X_train[0][embedding_dim*3: ])\n",
    "\n",
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(left == right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LIM = LIMDeepNeuralClassifier(\n",
    "    hidden_dim=embedding_dim*4, \n",
    "    hidden_activation=torch.nn.ReLU(), \n",
    "    num_layers=3,\n",
    "    input_dim=embedding_dim*4,\n",
    "    n_classes=2\n",
    "    )\n",
    "LIM_trainer = LIMTrainer(\n",
    "    LIM,\n",
    "    warm_start=True,\n",
    "    max_iter=10,\n",
    "    batch_size=64,\n",
    "    n_iter_no_change=10000,\n",
    "    shuffle_train=False,\n",
    "    eta=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rip out and replace the above, and keep it much simpler, since this doesn't seem to be working anyway.\n",
    "We'll start by eliminating the details of the rotatey stuff; since we'll want to do that in our own way later, anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a 3-layer relu to do the task:\n",
    "class ThreeLayerReLU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(ThreeLayerReLU, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.layer3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instantiating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1697],\n",
       "        [0.1398],\n",
       "        [0.1607],\n",
       "        ...,\n",
       "        [0.1744],\n",
       "        [0.1382],\n",
       "        [0.1990]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ThreeLayerReLU(16, 16, 16, 1)\n",
    "X_train = X_train.float()\n",
    "net(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(net, X_train, Y_train, epochs=100):\n",
    "    # Define a loss function and an optimizer\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "    for epoch in range(epochs):  \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(X_train)\n",
    "        # Compute the loss\n",
    "        loss = loss_function(outputs, Y_train)\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(net, X_train, y_train)\n",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, X_train, Y_train, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m outputs \u001b[39m=\u001b[39m net(X_train)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m loss \u001b[39m=\u001b[39m loss_function(outputs, Y_train)\n\u001b[1;32m     16\u001b[0m \u001b[39m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m     17\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/modules/loss.py:720\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 720\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[1;32m    721\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    722\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[1;32m    723\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/functional.py:3159\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3157\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m-> 3159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39;49msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[1;32m   3160\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[1;32m   3162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "#this is not working right now, not sure why, but I figured out enough to keep working on atticus' code directly\n",
    "#train(net, X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, hey, I figured out the problem! Running everything ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 10 of 10; error is 17.836314604996005"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LIMTrainer(\n",
       "\tbatch_size=64,\n",
       "\tmax_iter=10,\n",
       "\teta=0.001,\n",
       "\toptimizer_class=<class 'torch.optim.adam.Adam'>,\n",
       "\tl2_strength=0,\n",
       "\tgradient_accumulation_steps=1,\n",
       "\tmax_grad_norm=None,\n",
       "\tvalidation_fraction=0.1,\n",
       "\tearly_stopping=False,\n",
       "\tn_iter_no_change=10000,\n",
       "\twarm_start=True,\n",
       "\ttol=1e-05)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIM_trainer.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00     50000\\n           1       1.00      0.99      1.00     50000\\n\\n    accuracy                           1.00    100000\\n   macro avg       1.00      1.00      1.00    100000\\nweighted avg       1.00      1.00      1.00    100000\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds = LIM_trainer.predict(X_train, device=\"cpu\")\n",
    "train_preds = train_preds.cpu().numpy().astype('int64')\n",
    "\n",
    "classification_report(y_train, train_preds)\n",
    "#print(\"Train Results\")\n",
    "#print(classification_report(y_train, train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     50000\n",
      "           1       1.00      0.99      1.00     50000\n",
      "\n",
      "    accuracy                           1.00    100000\n",
      "   macro avg       1.00      1.00      1.00    100000\n",
      "weighted avg       1.00      1.00      1.00    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Results\")\n",
    "\n",
    "test_preds = LIM_trainer.predict(X_test, device=\"cpu\")\n",
    "test_preds = test_preds.cpu().numpy().astype('int64')\n",
    "\n",
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = X_train[0][: embedding_dim]\n",
    "b = X_train[1][: embedding_dim]\n",
    "c = X_train[2][: embedding_dim]\n",
    "\n",
    "X_same_different = torch.cat((a, a, b, c)).unsqueeze(0)\n",
    "\n",
    "X_different_same = torch.cat((a, b, c, c)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroing_get_coord = {\n",
    "    \"layer\": 1,\n",
    "    \"start\": 0, \n",
    "    \"end\": \n",
    "    embedding_dim*4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroing_intervention = [{\n",
    "    \"layer\": 1,\n",
    "    \"start\": 0,  \n",
    "    \"end\": embedding_dim, \n",
    "    \"intervention\": torch.zeros((1,embedding_dim))\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5062, 0.0083, 0.0000, 0.0974, 1.2192, 0.7053, 0.7218, 0.1774, 0.6769,\n",
       "         0.0000, 0.0206, 0.0000, 0.7264, 0.0000, 0.0000, 0.7720]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIM_trainer.model.retrieve_activations(X_same_different, zeroing_get_coord, None)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weirdly this doesn't seem to do anything? lol i wonder if we should fix that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIMDeepNeuralClassifier(\n",
       "  (hidden_activation): ReLU()\n",
       "  (loss): CrossEntropyLoss()\n",
       "  (model_layers): ModuleList(\n",
       "    (0): ActivationLayer(\n",
       "      (linear): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (1): ActivationLayer(\n",
       "      (linear): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (2): ActivationLayer(\n",
       "      (linear): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (3): Linear(in_features=16, out_features=2, bias=True)\n",
       "  )\n",
       "  (analysis_model): Sequential(\n",
       "    (0): ActivationLayer(\n",
       "      (linear): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (1): ParametrizedLinearLayer(\n",
       "      (parametrizations): ModuleDict(\n",
       "        (weight): ParametrizationList(\n",
       "          (0): _Orthogonal()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InverseLinearLayer(\n",
       "      (lin_layer): ParametrizedLinearLayer(\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _Orthogonal()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ActivationLayer(\n",
       "      (linear): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (4): ParametrizedLinearLayer(\n",
       "      (parametrizations): ModuleDict(\n",
       "        (weight): ParametrizationList(\n",
       "          (0): _Orthogonal()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): InverseLinearLayer(\n",
       "      (lin_layer): ParametrizedLinearLayer(\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _Orthogonal()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): ActivationLayer(\n",
       "      (linear): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (7): ParametrizedLinearLayer(\n",
       "      (parametrizations): ModuleDict(\n",
       "        (weight): ParametrizationList(\n",
       "          (0): _Orthogonal()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): InverseLinearLayer(\n",
       "      (lin_layer): ParametrizedLinearLayer(\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _Orthogonal()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): Linear(in_features=16, out_features=2, bias=True)\n",
       "  )\n",
       "  (normal_model): Sequential(\n",
       "    (0): ActivationLayer(\n",
       "      (linear): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (1): ActivationLayer(\n",
       "      (linear): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (2): ActivationLayer(\n",
       "      (linear): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (3): Linear(in_features=16, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIM_trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5062, 0.0083, 0.0000, 0.0974, 1.2192, 0.7053, 0.7218, 0.1774, 0.6769,\n",
       "         0.0000, 0.0206, 0.0000, 0.7264, 0.0000, 0.0000, 0.7720]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIM_trainer.model.retrieve_activations(X_same_different, zeroing_get_coord, zeroing_intervention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroing_output_coord = {\n",
    "    \"layer\": 3, \n",
    "    \"start\": 0, \n",
    "    \"end\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.5777, -2.5793]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIM_trainer.model.retrieve_activations(X_same_different, zeroing_output_coord, sets=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.7685, -1.8232]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIM_trainer.model.retrieve_activations(X_same_different, zeroing_output_coord, zeroing_intervention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii_coord = {\"layer\": 1, \"start\": 0, \"end\": embedding_dim}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5422, 0.0682, 0.0000, 0.6212]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intervention_get = LIM_trainer.model.retrieve_activations(X_different_same, ii_coord, None)\n",
    "\n",
    "intervention_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii_set = [{\n",
    "    \"layer\": 1, \n",
    "    \"start\": 0, \n",
    "    \"end\": embedding_dim, \n",
    "    \"intervention\": intervention_get}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5062, 0.0083, 0.0000, 0.0974]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIM_trainer.model.retrieve_activations(X_same_different, ii_coord, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5062, 0.0083, 0.0000, 0.0974]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIM_trainer.model.retrieve_activations(X_same_different, ii_coord, ii_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii_output_coord = {\"layer\": 3, \"start\": 0, \"end\": 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.5777, -2.5793]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIM_trainer.model.retrieve_activations(X_same_different, ii_output_coord, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.8578, -1.4604]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIM_trainer.model.retrieve_activations(X_same_different, ii_output_coord, ii_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alignment = {\n",
    "    \"V1\": {\"layer\": 1, \"start\": 0, \"end\": embedding_dim}, \n",
    "    \"V2\": {\"layer\": 1, \"start\": embedding_dim, \"end\": embedding_dim*2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interchange_intervention(model, base, source, get_coord, output_coord):\n",
    "    intervention = model.retrieve_activations(source, get_coord, None)\n",
    "    get_coord[\"intervention\"] = intervention\n",
    "    return model.retrieve_activations(base, output_coord, [get_coord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_coord = {\"layer\": 3, \"start\": 0, \"end\": 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.8578, -1.4604]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interchange_intervention(\n",
    "    LIM_trainer.model, \n",
    "    base=X_same_different, \n",
    "    source=X_different_same, \n",
    "    get_coord=ii_coord, \n",
    "    output_coord=output_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_input(tensor, embedding_dim):\n",
    "    return [tuple(tensor[0, embedding_dim*k:embedding_dim*(k+1)].flatten().tolist()) \n",
    "            for k in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_A(convert_input(X_same_different, embedding_dim), {})['output']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_A(convert_input(X_different_same, embedding_dim), {})['output']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_interchange_A(\n",
    "    convert_input(X_different_same, embedding_dim),\n",
    "    convert_input(X_same_different, embedding_dim),\n",
    "    variable=\"V1\")['output']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ii_evaluation(X_assess, model, variable, output_coord):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for base, source in itertools.product(X_assess, repeat=2):\n",
    "        base = base.unsqueeze(0)\n",
    "        source = source.unsqueeze(0)\n",
    "        # Run the high-level model with the intervention:\n",
    "        algorithm_output = compute_interchange_A(\n",
    "            convert_input(base, embedding_dim), \n",
    "            convert_input(source, embedding_dim), \n",
    "            variable)\n",
    "        # Get the high-level model's label:\n",
    "        labels.append(int(algorithm_output[\"output\"]))\n",
    "        # Run the neural model with the intervention:\n",
    "        network_output = interchange_intervention(\n",
    "            model, \n",
    "            base,\n",
    "            source,\n",
    "            alignment[variable],\n",
    "            output_coord)\n",
    "        # Get the neural model's prediction with the intervention:\n",
    "        pred = network_output.argmax(axis=1)\n",
    "        predictions.append(int(pred))\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.64      0.64      5048\n",
      "           1       0.64      0.64      0.64      4952\n",
      "\n",
      "    accuracy                           0.64     10000\n",
      "   macro avg       0.64      0.64      0.64     10000\n",
      "weighted avg       0.64      0.64      0.64     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(*ii_evaluation(X_test[: 100], LIM_trainer.model, \"V1\", output_coord)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.72      0.63      5048\n",
      "           1       0.59      0.41      0.48      4952\n",
      "\n",
      "    accuracy                           0.57     10000\n",
      "   macro avg       0.57      0.57      0.56     10000\n",
      "weighted avg       0.57      0.57      0.56     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(*ii_evaluation(X_test[: 100], LIM_trainer.model, \"V2\", output_coord)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not basis-aligned, I guess?\n",
    "task for tomorrow: implement this but with zen's rotational alignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
